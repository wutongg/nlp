{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SI 630 Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import html\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/xinye/Desktop/si618wn23-main/630project\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://cs.stanford.edu/~myasu/projects/scisumm_net/scisummnet_release1.1__20190413.zip\"\n",
    "response = requests.get(url)\n",
    "with zipfile.ZipFile(io.BytesIO(response.content)) as zipObj:\n",
    "     # Extract all the contents of zip file in different directory\n",
    "     zipObj.extractall(\"nlp_data\")\n",
    "     print(\"File is unzipped in nlp_data folder\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all raw text, break all papers into two parts -- Abstract and rest of document\n",
    "# first get all filepaths\n",
    "xmlfiles = []\n",
    "citations = []\n",
    "summary= []\n",
    "for subdir, dirs, files in os.walk(r'/home/xinye/nlp_data/scisummnet_release1.1__20190413/top1000_complete'):\n",
    "    for filename in files:\n",
    "        filepath = subdir + os.sep + filename\n",
    "        if filepath.endswith(\".xml\"):\n",
    "            xmlfiles.append(filepath)\n",
    "        if filepath.endswith(\".json\"):\n",
    "            citations.append(filepath)\n",
    "        if filepath.endswith(\".txt\"):\n",
    "            summary.append(filepath)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next parse all XML documents\n",
    "\n",
    "def parse_xml_abstract(fp):\n",
    "    \"\"\" parse an XML journal article into an abstract and the rest of the text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(fp)\n",
    "    except Exception as e:\n",
    "        return \"\",\"\",str(e)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    ab = []\n",
    "    bod = []\n",
    "    \n",
    "    for child in root:\n",
    "        if child.tag == \"ABSTRACT\":\n",
    "            for block in child:\n",
    "                ab.append(block.text)\n",
    "        else:\n",
    "            for block in child:\n",
    "                bod.append(block.text)\n",
    "                \n",
    "    #convert from list --> string\n",
    "    abstract = \"\\n\".join(ab)\n",
    "    body = \"\\n\".join(bod)\n",
    "    \n",
    "    #decode html entities\n",
    "    abstract = html.unescape(abstract)\n",
    "    body = html.unescape(body)\n",
    "    \n",
    "    return abstract,body,\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_cols = []\n",
    "for fpn in range(len(xmlfiles)):\n",
    "    ab,bod,err = parse_xml_abstract(xmlfiles[fpn])\n",
    "    if err:\n",
    "        #print(fp, err)\n",
    "        continue\n",
    "    f = open(citations[fpn]) \n",
    "\n",
    "    # returns JSON object as  \n",
    "    # a dictionary \n",
    "    data = json.load(f) \n",
    "    only_text = []\n",
    "    for entry in data:\n",
    "        only_text.append(entry['clean_text'])\n",
    "\n",
    "    raw_cols.append([ab,bod,only_text,xmlfiles[fpn]])\n",
    "\n",
    "df = pd.DataFrame(raw_cols, columns=[\"abstract\",\"body\",\"citations\", \"filepath\"])\n",
    "# only use the first 20 papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all summary text using the summary path\n",
    "\n",
    "summary_text = []\n",
    "for fpn in range(len(summary)):\n",
    "    f = open(summary[fpn]) \n",
    "    data = f.read()\n",
    "    summary_text.append(data)\n",
    "\n",
    "df['summary'] = summary_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>body</th>\n",
       "      <th>citations</th>\n",
       "      <th>filepath</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2: Test set performance of our systems: and ou...</td>\n",
       "      <td>The open source Moses (Koehn et al., 2007) MT ...</td>\n",
       "      <td>[Factored translation models have also been us...</td>\n",
       "      <td>/home/xinye/nlp_data/scisummnet_release1.1__20...</td>\n",
       "      <td>Experiments in Domain Adaptation for Statistic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>At present, adapting an Information Extraction...</td>\n",
       "      <td>Most of the world’s information is recorded, p...</td>\n",
       "      <td>[Our work is related to previous work on domai...</td>\n",
       "      <td>/home/xinye/nlp_data/scisummnet_release1.1__20...</td>\n",
       "      <td>On-Demand Information Extraction\\nAt present, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In this paper, we have proposed novel methods ...</td>\n",
       "      <td>In this paper, we have proposed novel methods ...</td>\n",
       "      <td>[These are much finer grained than Penn Treeba...</td>\n",
       "      <td>/home/xinye/nlp_data/scisummnet_release1.1__20...</td>\n",
       "      <td>Supertagging: An Approach To Almost Parsing\\nI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Statistical machine translation systems are us...</td>\n",
       "      <td>In statistical machine translation (SMT), tran...</td>\n",
       "      <td>[Transductive learning method (Ueffing et al, ...</td>\n",
       "      <td>/home/xinye/nlp_data/scisummnet_release1.1__20...</td>\n",
       "      <td>Transductive learning for statistical machine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We combine the strengths of Bayesian modeling ...</td>\n",
       "      <td>Most state-of-the-art statistical machine tran...</td>\n",
       "      <td>[This is in line with earlier work on consiste...</td>\n",
       "      <td>/home/xinye/nlp_data/scisummnet_release1.1__20...</td>\n",
       "      <td>Bayesian Learning of Non-Compositional Phrases...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  \\\n",
       "0  2: Test set performance of our systems: and ou...   \n",
       "1  At present, adapting an Information Extraction...   \n",
       "2  In this paper, we have proposed novel methods ...   \n",
       "3  Statistical machine translation systems are us...   \n",
       "4  We combine the strengths of Bayesian modeling ...   \n",
       "\n",
       "                                                body  \\\n",
       "0  The open source Moses (Koehn et al., 2007) MT ...   \n",
       "1  Most of the world’s information is recorded, p...   \n",
       "2  In this paper, we have proposed novel methods ...   \n",
       "3  In statistical machine translation (SMT), tran...   \n",
       "4  Most state-of-the-art statistical machine tran...   \n",
       "\n",
       "                                           citations  \\\n",
       "0  [Factored translation models have also been us...   \n",
       "1  [Our work is related to previous work on domai...   \n",
       "2  [These are much finer grained than Penn Treeba...   \n",
       "3  [Transductive learning method (Ueffing et al, ...   \n",
       "4  [This is in line with earlier work on consiste...   \n",
       "\n",
       "                                            filepath  \\\n",
       "0  /home/xinye/nlp_data/scisummnet_release1.1__20...   \n",
       "1  /home/xinye/nlp_data/scisummnet_release1.1__20...   \n",
       "2  /home/xinye/nlp_data/scisummnet_release1.1__20...   \n",
       "3  /home/xinye/nlp_data/scisummnet_release1.1__20...   \n",
       "4  /home/xinye/nlp_data/scisummnet_release1.1__20...   \n",
       "\n",
       "                                             summary  \n",
       "0  Experiments in Domain Adaptation for Statistic...  \n",
       "1  On-Demand Information Extraction\\nAt present, ...  \n",
       "2  Supertagging: An Approach To Almost Parsing\\nI...  \n",
       "3  Transductive learning for statistical machine ...  \n",
       "4  Bayesian Learning of Non-Compositional Phrases...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop rows with missing values or empty abstracts\n",
    "df = df.dropna()\n",
    "df = df[df.abstract != \"\"]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('summaries_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(924, 6)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('summaries_sample.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data in 80:10:10 for train:valid:test dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "in_df = df.sample(len(df), random_state=630)\n",
    "train_df, rem_df = train_test_split(in_df, train_size=0.8)\n",
    "val_df, test_df = train_test_split(rem_df, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive model 1:  Textrank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the experiment, we found it is hard to let the pagerank algorithms converge. So we decided to use shorter sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from rouge import Rouge\n",
    "\n",
    "def extractive_summarizer(text, num_sentences=3):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    summarizer = TextRankSummarizer()\n",
    "    summary = summarizer(parser.document, num_sentences)\n",
    "    \n",
    "    summarized_text = '. '.join([str(sentence) for sentence in summary]) + '.'\n",
    "    return summarized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We asked a subject to judge usefulness in three grades; A) very useful – for the query, many people might want to use this table for the further investigation of the topic, B) useful – at least, for some purpose, some people might want to use this table for further investigation and C) not useful – no one will be interested in using this table for further investigation.. Compared to the results in the ‘useful’ category, the tables for these two topics have more slots filled and the NE types of the fillers have fewer mistakes.. However, the results are limited to a pair of participants and because of the nature of the procedure, the discovered relations are static relations like a country and its presidents rather than events..\n"
     ]
    }
   ],
   "source": [
    "# Test the summarizer\n",
    "text = df['body'][1]\n",
    "print(extractive_summarizer(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(df, num_sentences=3):\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        text = row['body']\n",
    "        reference = row['summary']\n",
    "        summary = extractive_summarizer(text, num_sentences)\n",
    "        \n",
    "        score = rouge.get_scores(summary, reference, avg=True)\n",
    "        rouge_scores.append(score)\n",
    "    \n",
    "    return rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_scores = evaluate(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'f': 0.3115264747682962,\n",
       "   'p': 0.29411764705882354,\n",
       "   'r': 0.33112582781456956},\n",
       "  'rouge-2': {'f': 0.06896551225911732,\n",
       "   'p': 0.0650887573964497,\n",
       "   'r': 0.07333333333333333},\n",
       "  'rouge-l': {'f': 0.18947367921052644,\n",
       "   'p': 0.18947368421052632,\n",
       "   'r': 0.18947368421052632}},\n",
       " {'rouge-1': {'f': 0.2686567115430609,\n",
       "   'p': 0.23195876288659795,\n",
       "   'r': 0.3191489361702128},\n",
       "  'rouge-2': {'f': 0.0240240191506832,\n",
       "   'p': 0.02072538860103627,\n",
       "   'r': 0.02857142857142857},\n",
       "  'rouge-l': {'f': 0.21008402864875372,\n",
       "   'p': 0.1937984496124031,\n",
       "   'r': 0.22935779816513763}},\n",
       " {'rouge-1': {'f': 0.24365481744337664,\n",
       "   'p': 0.2857142857142857,\n",
       "   'r': 0.21238938053097345},\n",
       "  'rouge-2': {'f': 0.010204076742115006,\n",
       "   'p': 0.011976047904191617,\n",
       "   'r': 0.008888888888888889},\n",
       "  'rouge-l': {'f': 0.16733067246615146,\n",
       "   'p': 0.20588235294117646,\n",
       "   'r': 0.14093959731543623}},\n",
       " {'rouge-1': {'f': 0.2787286014832528,\n",
       "   'p': 0.3313953488372093,\n",
       "   'r': 0.24050632911392406},\n",
       "  'rouge-2': {'f': 0.06388205900959296,\n",
       "   'p': 0.07602339181286549,\n",
       "   'r': 0.05508474576271186},\n",
       "  'rouge-l': {'f': 0.18881118395202712,\n",
       "   'p': 0.226890756302521,\n",
       "   'r': 0.16167664670658682}},\n",
       " {'rouge-1': {'f': 0.26277371775667335,\n",
       "   'p': 0.22641509433962265,\n",
       "   'r': 0.3130434782608696},\n",
       "  'rouge-2': {'f': 0.014705877483781889,\n",
       "   'p': 0.012658227848101266,\n",
       "   'r': 0.017543859649122806},\n",
       "  'rouge-l': {'f': 0.22727272231404969, 'p': 0.20833333333333334, 'r': 0.25}},\n",
       " {'rouge-1': {'f': 0.21637426448616676,\n",
       "   'p': 0.16517857142857142,\n",
       "   'r': 0.3135593220338983},\n",
       "  'rouge-2': {'f': 0.035294113133045556,\n",
       "   'p': 0.026905829596412557,\n",
       "   'r': 0.05128205128205128},\n",
       "  'rouge-l': {'f': 0.2352941128732009,\n",
       "   'p': 0.19402985074626866,\n",
       "   'r': 0.2988505747126437}},\n",
       " {'rouge-1': {'f': 0.1825396775399976, 'p': 0.18110236220472442, 'r': 0.184},\n",
       "  'rouge-2': {'f': 0.03999999500032063,\n",
       "   'p': 0.03968253968253968,\n",
       "   'r': 0.04032258064516129},\n",
       "  'rouge-l': {'f': 0.12429378034664389,\n",
       "   'p': 0.13580246913580246,\n",
       "   'r': 0.11458333333333333}},\n",
       " {'rouge-1': {'f': 0.31818181321813666,\n",
       "   'p': 0.2931937172774869,\n",
       "   'r': 0.34782608695652173},\n",
       "  'rouge-2': {'f': 0.028571423608164125,\n",
       "   'p': 0.02631578947368421,\n",
       "   'r': 0.03125},\n",
       "  'rouge-l': {'f': 0.2109704591457923,\n",
       "   'p': 0.20161290322580644,\n",
       "   'r': 0.22123893805309736}},\n",
       " {'rouge-1': {'f': 0.07689883286890482,\n",
       "   'p': 0.040693795863909275,\n",
       "   'r': 0.6971428571428572},\n",
       "  'rouge-2': {'f': 0.01892147483789206,\n",
       "   'p': 0.01001001001001001,\n",
       "   'r': 0.1724137931034483},\n",
       "  'rouge-l': {'f': 0.0984665034837254,\n",
       "   'p': 0.05456171735241502,\n",
       "   'r': 0.5041322314049587}},\n",
       " {'rouge-1': {'f': 0.24422441759522495, 'p': 0.20786516853932585, 'r': 0.296},\n",
       "  'rouge-2': {'f': 0.03986710478957244,\n",
       "   'p': 0.03389830508474576,\n",
       "   'r': 0.04838709677419355},\n",
       "  'rouge-l': {'f': 0.20212765457446819,\n",
       "   'p': 0.20212765957446807,\n",
       "   'r': 0.20212765957446807}},\n",
       " {'rouge-1': {'f': 0.20833332856805567,\n",
       "   'p': 0.17123287671232876,\n",
       "   'r': 0.26595744680851063},\n",
       "  'rouge-2': {'f': 0.00840335658322422,\n",
       "   'p': 0.006896551724137931,\n",
       "   'r': 0.010752688172043012},\n",
       "  'rouge-l': {'f': 0.16470587748858145,\n",
       "   'p': 0.1414141414141414,\n",
       "   'r': 0.19718309859154928}},\n",
       " {'rouge-1': {'f': 0.22026431265607727,\n",
       "   'p': 0.16835016835016836,\n",
       "   'r': 0.3184713375796178},\n",
       "  'rouge-2': {'f': 0.017699110523926285,\n",
       "   'p': 0.013513513513513514,\n",
       "   'r': 0.02564102564102564},\n",
       "  'rouge-l': {'f': 0.1830985867875919,\n",
       "   'p': 0.15028901734104047,\n",
       "   'r': 0.23423423423423423}},\n",
       " {'rouge-1': {'f': 0.30859374502754217,\n",
       "   'p': 0.3333333333333333,\n",
       "   'r': 0.2872727272727273},\n",
       "  'rouge-2': {'f': 0.03921568130226899,\n",
       "   'p': 0.0423728813559322,\n",
       "   'r': 0.0364963503649635},\n",
       "  'rouge-l': {'f': 0.21176470088858146,\n",
       "   'p': 0.21951219512195122,\n",
       "   'r': 0.20454545454545456}},\n",
       " {'rouge-1': {'f': 0.24999999500779094,\n",
       "   'p': 0.2602739726027397,\n",
       "   'r': 0.24050632911392406},\n",
       "  'rouge-2': {'f': 0.03973509434564337,\n",
       "   'p': 0.041379310344827586,\n",
       "   'r': 0.03821656050955414},\n",
       "  'rouge-l': {'f': 0.22680411871134032,\n",
       "   'p': 0.2268041237113402,\n",
       "   'r': 0.2268041237113402}},\n",
       " {'rouge-1': {'f': 0.23728813084857486,\n",
       "   'p': 0.1935483870967742,\n",
       "   'r': 0.30656934306569344},\n",
       "  'rouge-2': {'f': 0.0397727225309923,\n",
       "   'p': 0.032407407407407406,\n",
       "   'r': 0.051470588235294115},\n",
       "  'rouge-l': {'f': 0.18518518022805225,\n",
       "   'p': 0.1694915254237288,\n",
       "   'r': 0.20408163265306123}},\n",
       " {'rouge-1': {'f': 0.28274427805031965,\n",
       "   'p': 0.22666666666666666,\n",
       "   'r': 0.3756906077348066},\n",
       "  'rouge-2': {'f': 0.06680584082008044,\n",
       "   'p': 0.05351170568561873,\n",
       "   'r': 0.08888888888888889},\n",
       "  'rouge-l': {'f': 0.17589576059968823,\n",
       "   'p': 0.15168539325842698,\n",
       "   'r': 0.20930232558139536}},\n",
       " {'rouge-1': {'f': 0.21232876237872972,\n",
       "   'p': 0.17318435754189945,\n",
       "   'r': 0.2743362831858407},\n",
       "  'rouge-2': {'f': 0.006896546983118598,\n",
       "   'p': 0.0056179775280898875,\n",
       "   'r': 0.008928571428571428},\n",
       "  'rouge-l': {'f': 0.14851484656798367,\n",
       "   'p': 0.13157894736842105,\n",
       "   'r': 0.17045454545454544}},\n",
       " {'rouge-1': {'f': 0.315999995242,\n",
       "   'p': 0.40512820512820513,\n",
       "   'r': 0.25901639344262295},\n",
       "  'rouge-2': {'f': 0.07228915187045402,\n",
       "   'p': 0.09278350515463918,\n",
       "   'r': 0.05921052631578947},\n",
       "  'rouge-l': {'f': 0.17254901468173794,\n",
       "   'p': 0.19642857142857142,\n",
       "   'r': 0.15384615384615385}},\n",
       " {'rouge-1': {'f': 0.24703087417087471,\n",
       "   'p': 0.19771863117870722,\n",
       "   'r': 0.3291139240506329},\n",
       "  'rouge-2': {'f': 0.02863961345241901,\n",
       "   'p': 0.022900763358778626,\n",
       "   'r': 0.03821656050955414},\n",
       "  'rouge-l': {'f': 0.14285713800957436,\n",
       "   'p': 0.12162162162162163,\n",
       "   'r': 0.17307692307692307}},\n",
       " {'rouge-1': {'f': 0.28496041720149545,\n",
       "   'p': 0.2621359223300971,\n",
       "   'r': 0.31213872832369943},\n",
       "  'rouge-2': {'f': 0.04774535312849646,\n",
       "   'p': 0.04390243902439024,\n",
       "   'r': 0.05232558139534884},\n",
       "  'rouge-l': {'f': 0.2063492013495214, 'p': 0.2047244094488189, 'r': 0.208}},\n",
       " {'rouge-1': {'f': 0.26378896388845763,\n",
       "   'p': 0.2972972972972973,\n",
       "   'r': 0.23706896551724138},\n",
       "  'rouge-2': {'f': 0.0433734890400354,\n",
       "   'p': 0.04891304347826087,\n",
       "   'r': 0.03896103896103896},\n",
       "  'rouge-l': {'f': 0.15079364585537933,\n",
       "   'p': 0.16964285714285715,\n",
       "   'r': 0.1357142857142857}},\n",
       " {'rouge-1': {'f': 0.2941176420791283,\n",
       "   'p': 0.2764976958525346,\n",
       "   'r': 0.31413612565445026},\n",
       "  'rouge-2': {'f': 0.0492610787643481,\n",
       "   'p': 0.046296296296296294,\n",
       "   'r': 0.05263157894736842},\n",
       "  'rouge-l': {'f': 0.1992337114751693, 'p': 0.2, 'r': 0.1984732824427481}},\n",
       " {'rouge-1': {'f': 0.22580644685348603,\n",
       "   'p': 0.18518518518518517,\n",
       "   'r': 0.2892561983471074},\n",
       "  'rouge-2': {'f': 0.025974021217744166,\n",
       "   'p': 0.02127659574468085,\n",
       "   'r': 0.03333333333333333},\n",
       "  'rouge-l': {'f': 0.18579234473409195,\n",
       "   'p': 0.17894736842105263,\n",
       "   'r': 0.19318181818181818}},\n",
       " {'rouge-1': {'f': 0.22545454053553732, 'p': 0.2, 'r': 0.25833333333333336},\n",
       "  'rouge-2': {'f': 0.014652009734199075,\n",
       "   'p': 0.012987012987012988,\n",
       "   'r': 0.01680672268907563},\n",
       "  'rouge-l': {'f': 0.19095476887250337,\n",
       "   'p': 0.18627450980392157,\n",
       "   'r': 0.1958762886597938}},\n",
       " {'rouge-1': {'f': 0.24472573392440675, 'p': 0.18471337579617833, 'r': 0.3625},\n",
       "  'rouge-2': {'f': 0.02118643621023148,\n",
       "   'p': 0.01597444089456869,\n",
       "   'r': 0.031446540880503145},\n",
       "  'rouge-l': {'f': 0.17605633353030167,\n",
       "   'p': 0.13368983957219252,\n",
       "   'r': 0.25773195876288657}},\n",
       " {'rouge-1': {'f': 0.195402294614216,\n",
       "   'p': 0.14049586776859505,\n",
       "   'r': 0.32075471698113206},\n",
       "  'rouge-2': {'f': 0.005780342593307245,\n",
       "   'p': 0.004149377593360996,\n",
       "   'r': 0.009523809523809525},\n",
       "  'rouge-l': {'f': 0.12785387674652338,\n",
       "   'p': 0.0979020979020979,\n",
       "   'r': 0.18421052631578946}},\n",
       " {'rouge-1': {'f': 0.3399433377938994,\n",
       "   'p': 0.32085561497326204,\n",
       "   'r': 0.3614457831325301},\n",
       "  'rouge-2': {'f': 0.045584040601943716,\n",
       "   'p': 0.043010752688172046,\n",
       "   'r': 0.048484848484848485},\n",
       "  'rouge-l': {'f': 0.2133333283334322,\n",
       "   'p': 0.21428571428571427,\n",
       "   'r': 0.21238938053097345}},\n",
       " {'rouge-1': {'f': 0.3442622901478098,\n",
       "   'p': 0.3088235294117647,\n",
       "   'r': 0.3888888888888889},\n",
       "  'rouge-2': {'f': 0.09917354878594384,\n",
       "   'p': 0.08888888888888889,\n",
       "   'r': 0.11214953271028037},\n",
       "  'rouge-l': {'f': 0.22222221723952337,\n",
       "   'p': 0.20987654320987653,\n",
       "   'r': 0.2361111111111111}},\n",
       " {'rouge-1': {'f': 0.2222222173746537,\n",
       "   'p': 0.1891891891891892,\n",
       "   'r': 0.2692307692307692},\n",
       "  'rouge-2': {'f': 0.02139036948726124,\n",
       "   'p': 0.01818181818181818,\n",
       "   'r': 0.025974025974025976},\n",
       "  'rouge-l': {'f': 0.15714285219285729,\n",
       "   'p': 0.14285714285714285,\n",
       "   'r': 0.1746031746031746}},\n",
       " {'rouge-1': {'f': 0.2652519850502009,\n",
       "   'p': 0.19455252918287938,\n",
       "   'r': 0.4166666666666667},\n",
       "  'rouge-2': {'f': 0.04799999566734261,\n",
       "   'p': 0.03515625,\n",
       "   'r': 0.07563025210084033},\n",
       "  'rouge-l': {'f': 0.16949152068550716,\n",
       "   'p': 0.13793103448275862,\n",
       "   'r': 0.21978021978021978}},\n",
       " {'rouge-1': {'f': 0.17204300728595798,\n",
       "   'p': 0.11072664359861592,\n",
       "   'r': 0.3855421686746988},\n",
       "  'rouge-2': {'f': 0.037837834387728586,\n",
       "   'p': 0.024305555555555556,\n",
       "   'r': 0.08536585365853659},\n",
       "  'rouge-l': {'f': 0.15789473269198226,\n",
       "   'p': 0.11180124223602485,\n",
       "   'r': 0.26865671641791045}},\n",
       " {'rouge-1': {'f': 0.18532818066278092, 'p': 0.147239263803681, 'r': 0.25},\n",
       "  'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
       "  'rouge-l': {'f': 0.11627906484924305,\n",
       "   'p': 0.10309278350515463,\n",
       "   'r': 0.13333333333333333}},\n",
       " {'rouge-1': {'f': 0.2116788275646013,\n",
       "   'p': 0.16292134831460675,\n",
       "   'r': 0.3020833333333333},\n",
       "  'rouge-2': {'f': 0.007352936630895974,\n",
       "   'p': 0.005649717514124294,\n",
       "   'r': 0.010526315789473684},\n",
       "  'rouge-l': {'f': 0.1530612197193879,\n",
       "   'p': 0.12605042016806722,\n",
       "   'r': 0.19480519480519481}},\n",
       " {'rouge-1': {'f': 0.12138728025844506,\n",
       "   'p': 0.07420494699646643,\n",
       "   'r': 0.3333333333333333},\n",
       "  'rouge-2': {'f': 0.014492750656375317, 'p': 0.008849557522123894, 'r': 0.04},\n",
       "  'rouge-l': {'f': 0.1393939357208449, 'p': 0.092, 'r': 0.2875}},\n",
       " {'rouge-1': {'f': 0.10650887245544634,\n",
       "   'p': 0.06716417910447761,\n",
       "   'r': 0.2571428571428571},\n",
       "  'rouge-2': {'f': 0.017857139593431718,\n",
       "   'p': 0.011235955056179775,\n",
       "   'r': 0.043478260869565216},\n",
       "  'rouge-l': {'f': 0.08888888506232116,\n",
       "   'p': 0.059880239520958084,\n",
       "   'r': 0.1724137931034483}},\n",
       " {'rouge-1': {'f': 0.1290322544982311,\n",
       "   'p': 0.08403361344537816,\n",
       "   'r': 0.2777777777777778},\n",
       "  'rouge-2': {'f': 0.012987009439408963,\n",
       "   'p': 0.008438818565400843,\n",
       "   'r': 0.028169014084507043},\n",
       "  'rouge-l': {'f': 0.1452513925732656,\n",
       "   'p': 0.1015625,\n",
       "   'r': 0.2549019607843137}},\n",
       " {'rouge-1': {'f': 0.16393442230361083,\n",
       "   'p': 0.1120162932790224,\n",
       "   'r': 0.3055555555555556},\n",
       "  'rouge-2': {'f': 0.011958142567830028,\n",
       "   'p': 0.00816326530612245,\n",
       "   'r': 0.0223463687150838},\n",
       "  'rouge-l': {'f': 0.15718156719031162,\n",
       "   'p': 0.12340425531914893,\n",
       "   'r': 0.21641791044776118}},\n",
       " {'rouge-1': {'f': 0.2823529363025952,\n",
       "   'p': 0.3356643356643357,\n",
       "   'r': 0.2436548223350254},\n",
       "  'rouge-2': {'f': 0.06508875252407166,\n",
       "   'p': 0.07746478873239436,\n",
       "   'r': 0.05612244897959184},\n",
       "  'rouge-l': {'f': 0.24365481753098517,\n",
       "   'p': 0.3037974683544304,\n",
       "   'r': 0.2033898305084746}},\n",
       " {'rouge-1': {'f': 0.19941348475159326,\n",
       "   'p': 0.18888888888888888,\n",
       "   'r': 0.2111801242236025},\n",
       "  'rouge-2': {'f': 0.04129793011895189,\n",
       "   'p': 0.03910614525139665,\n",
       "   'r': 0.04375},\n",
       "  'rouge-l': {'f': 0.20792078716204304,\n",
       "   'p': 0.23863636363636365,\n",
       "   'r': 0.18421052631578946}},\n",
       " {'rouge-1': {'f': 0.3183856453618613,\n",
       "   'p': 0.27307692307692305,\n",
       "   'r': 0.3817204301075269},\n",
       "  'rouge-2': {'f': 0.05855855369744785,\n",
       "   'p': 0.05019305019305019,\n",
       "   'r': 0.07027027027027027},\n",
       "  'rouge-l': {'f': 0.22368420558171756, 'p': 0.20238095238095238, 'r': 0.25}},\n",
       " {'rouge-1': {'f': 0.24767801371047365,\n",
       "   'p': 0.2127659574468085,\n",
       "   'r': 0.2962962962962963},\n",
       "  'rouge-2': {'f': 0.018691583921352957,\n",
       "   'p': 0.016042780748663103,\n",
       "   'r': 0.022388059701492536},\n",
       "  'rouge-l': {'f': 0.16964285223254158,\n",
       "   'p': 0.14960629921259844,\n",
       "   'r': 0.1958762886597938}},\n",
       " {'rouge-1': {'f': 0.27796609675012934,\n",
       "   'p': 0.25153374233128833,\n",
       "   'r': 0.3106060606060606},\n",
       "  'rouge-2': {'f': 0.05460750358839402,\n",
       "   'p': 0.04938271604938271,\n",
       "   'r': 0.061068702290076333},\n",
       "  'rouge-l': {'f': 0.26732672768110977,\n",
       "   'p': 0.2571428571428571,\n",
       "   'r': 0.27835051546391754}},\n",
       " {'rouge-1': {'f': 0.30054644313580586,\n",
       "   'p': 0.2736318407960199,\n",
       "   'r': 0.3333333333333333},\n",
       "  'rouge-2': {'f': 0.07692307197198438, 'p': 0.07, 'r': 0.08536585365853659},\n",
       "  'rouge-l': {'f': 0.19199999508192012,\n",
       "   'p': 0.1702127659574468,\n",
       "   'r': 0.22018348623853212}},\n",
       " {'rouge-1': {'f': 0.30246913102271, 'p': 0.25, 'r': 0.3828125},\n",
       "  'rouge-2': {'f': 0.05590061634099806,\n",
       "   'p': 0.046153846153846156,\n",
       "   'r': 0.07086614173228346},\n",
       "  'rouge-l': {'f': 0.25837320082507276,\n",
       "   'p': 0.2288135593220339,\n",
       "   'r': 0.2967032967032967}},\n",
       " {'rouge-1': {'f': 0.2756756706762601,\n",
       "   'p': 0.2727272727272727,\n",
       "   'r': 0.2786885245901639},\n",
       "  'rouge-2': {'f': 0.0217391254353745,\n",
       "   'p': 0.021505376344086023,\n",
       "   'r': 0.02197802197802198},\n",
       "  'rouge-l': {'f': 0.2509803871846214,\n",
       "   'p': 0.2711864406779661,\n",
       "   'r': 0.23357664233576642}},\n",
       " {'rouge-1': {'f': 0.22222221748103363,\n",
       "   'p': 0.1810344827586207,\n",
       "   'r': 0.2876712328767123},\n",
       "  'rouge-2': {'f': 0.03191488887859396,\n",
       "   'p': 0.025974025974025976,\n",
       "   'r': 0.041379310344827586},\n",
       "  'rouge-l': {'f': 0.1685823706551579,\n",
       "   'p': 0.14193548387096774,\n",
       "   'r': 0.20754716981132076}},\n",
       " {'rouge-1': {'f': 0.24523160264579894,\n",
       "   'p': 0.26011560693641617,\n",
       "   'r': 0.23195876288659795},\n",
       "  'rouge-2': {'f': 0.027397255290524454,\n",
       "   'p': 0.029069767441860465,\n",
       "   'r': 0.025906735751295335},\n",
       "  'rouge-l': {'f': 0.26086956031417774,\n",
       "   'p': 0.30303030303030304,\n",
       "   'r': 0.22900763358778625}},\n",
       " {'rouge-1': {'f': 0.24046920372201824,\n",
       "   'p': 0.18222222222222223,\n",
       "   'r': 0.35344827586206895},\n",
       "  'rouge-2': {'f': 0.04719763563491485,\n",
       "   'p': 0.03571428571428571,\n",
       "   'r': 0.06956521739130435},\n",
       "  'rouge-l': {'f': 0.17213114288497727,\n",
       "   'p': 0.13636363636363635,\n",
       "   'r': 0.23333333333333334}},\n",
       " {'rouge-1': {'f': 0.33168316332908543,\n",
       "   'p': 0.3160377358490566,\n",
       "   'r': 0.3489583333333333},\n",
       "  'rouge-2': {'f': 0.04975123879347096,\n",
       "   'p': 0.04739336492890995,\n",
       "   'r': 0.05235602094240838},\n",
       "  'rouge-l': {'f': 0.2598425146927895, 'p': 0.27049180327868855, 'r': 0.25}},\n",
       " {'rouge-1': {'f': 0.26984126499370126,\n",
       "   'p': 0.3269230769230769,\n",
       "   'r': 0.22972972972972974},\n",
       "  'rouge-2': {'f': 0.042553186643419526,\n",
       "   'p': 0.05161290322580645,\n",
       "   'r': 0.03619909502262444},\n",
       "  'rouge-l': {'f': 0.15384614921405051,\n",
       "   'p': 0.2111111111111111,\n",
       "   'r': 0.12101910828025478}},\n",
       " {'rouge-1': {'f': 0.2727272677333855,\n",
       "   'p': 0.2635135135135135,\n",
       "   'r': 0.2826086956521739},\n",
       "  'rouge-2': {'f': 0.0563380231752137,\n",
       "   'p': 0.05442176870748299,\n",
       "   'r': 0.058394160583941604},\n",
       "  'rouge-l': {'f': 0.12698412198426715,\n",
       "   'p': 0.12631578947368421,\n",
       "   'r': 0.1276595744680851}},\n",
       " {'rouge-1': {'f': 0.20817843366233202,\n",
       "   'p': 0.20588235294117646,\n",
       "   'r': 0.21052631578947367},\n",
       "  'rouge-2': {'f': 0.014981268408872601,\n",
       "   'p': 0.014814814814814815,\n",
       "   'r': 0.015151515151515152},\n",
       "  'rouge-l': {'f': 0.1818181768183656,\n",
       "   'p': 0.18072289156626506,\n",
       "   'r': 0.18292682926829268}},\n",
       " {'rouge-1': {'f': 0.21301774653601074,\n",
       "   'p': 0.1925133689839572,\n",
       "   'r': 0.23841059602649006},\n",
       "  'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
       "  'rouge-l': {'f': 0.13973798627104764,\n",
       "   'p': 0.13559322033898305,\n",
       "   'r': 0.14414414414414414}},\n",
       " {'rouge-1': {'f': 0.1640378505941945,\n",
       "   'p': 0.11926605504587157,\n",
       "   'r': 0.26262626262626265},\n",
       "  'rouge-2': {'f': 0.025396821110406365,\n",
       "   'p': 0.018433179723502304,\n",
       "   'r': 0.04081632653061224},\n",
       "  'rouge-l': {'f': 0.11578946886371211,\n",
       "   'p': 0.09734513274336283,\n",
       "   'r': 0.14285714285714285}},\n",
       " {'rouge-1': {'f': 0.2981366410832916,\n",
       "   'p': 0.25806451612903225,\n",
       "   'r': 0.35294117647058826},\n",
       "  'rouge-2': {'f': 0.01874999512207158,\n",
       "   'p': 0.016216216216216217,\n",
       "   'r': 0.022222222222222223},\n",
       "  'rouge-l': {'f': 0.21463414141582401,\n",
       "   'p': 0.19130434782608696,\n",
       "   'r': 0.24444444444444444}},\n",
       " {'rouge-1': {'f': 0.13925925504158038,\n",
       "   'p': 0.09978768577494693,\n",
       "   'r': 0.23039215686274508},\n",
       "  'rouge-2': {'f': 0.01783060499946007,\n",
       "   'p': 0.01276595744680851,\n",
       "   'r': 0.029556650246305417},\n",
       "  'rouge-l': {'f': 0.07505518378531176,\n",
       "   'p': 0.050746268656716415,\n",
       "   'r': 0.1440677966101695}},\n",
       " {'rouge-1': {'f': 0.2925531867509337,\n",
       "   'p': 0.23809523809523808,\n",
       "   'r': 0.3793103448275862},\n",
       "  'rouge-2': {'f': 0.07486630542480513,\n",
       "   'p': 0.06086956521739131,\n",
       "   'r': 0.09722222222222222},\n",
       "  'rouge-l': {'f': 0.22429906046423279,\n",
       "   'p': 0.20512820512820512,\n",
       "   'r': 0.24742268041237114}},\n",
       " {'rouge-1': {'f': 0.18633539933162313,\n",
       "   'p': 0.1382488479262673,\n",
       "   'r': 0.2857142857142857},\n",
       "  'rouge-2': {'f': 0.018749995612501024,\n",
       "   'p': 0.013888888888888888,\n",
       "   'r': 0.028846153846153848},\n",
       "  'rouge-l': {'f': 0.1390374282593156, 'p': 0.12149532710280374, 'r': 0.1625}},\n",
       " {'rouge-1': {'f': 0.3116883068909176,\n",
       "   'p': 0.2594594594594595,\n",
       "   'r': 0.3902439024390244},\n",
       "  'rouge-2': {'f': 0.07843136775428282,\n",
       "   'p': 0.06521739130434782,\n",
       "   'r': 0.09836065573770492},\n",
       "  'rouge-l': {'f': 0.26415093846030624,\n",
       "   'p': 0.23728813559322035,\n",
       "   'r': 0.2978723404255319}},\n",
       " {'rouge-1': {'f': 0.2576271136455043,\n",
       "   'p': 0.2620689655172414,\n",
       "   'r': 0.25333333333333335},\n",
       "  'rouge-2': {'f': 0.04095562640077406,\n",
       "   'p': 0.041666666666666664,\n",
       "   'r': 0.040268456375838924},\n",
       "  'rouge-l': {'f': 0.18627450482747035, 'p': 0.2, 'r': 0.1743119266055046}},\n",
       " {'rouge-1': {'f': 0.2352941126729037,\n",
       "   'p': 0.2535211267605634,\n",
       "   'r': 0.21951219512195122},\n",
       "  'rouge-2': {'f': 0.0065789423946107555,\n",
       "   'p': 0.0070921985815602835,\n",
       "   'r': 0.006134969325153374},\n",
       "  'rouge-l': {'f': 0.15533980094589517,\n",
       "   'p': 0.1839080459770115,\n",
       "   'r': 0.13445378151260504}},\n",
       " {'rouge-1': {'f': 0.3854166621549479,\n",
       "   'p': 0.5606060606060606,\n",
       "   'r': 0.29365079365079366},\n",
       "  'rouge-2': {'f': 0.06282722062429791,\n",
       "   'p': 0.0916030534351145,\n",
       "   'r': 0.04780876494023904},\n",
       "  'rouge-l': {'f': 0.25941422138267894, 'p': 0.36904761904761907, 'r': 0.2}},\n",
       " {'rouge-1': {'f': 0.2078651635551068,\n",
       "   'p': 0.19680851063829788,\n",
       "   'r': 0.22023809523809523},\n",
       "  'rouge-2': {'f': 0.011299430044210438,\n",
       "   'p': 0.0106951871657754,\n",
       "   'r': 0.011976047904191617},\n",
       "  'rouge-l': {'f': 0.13333332833343228,\n",
       "   'p': 0.13274336283185842,\n",
       "   'r': 0.13392857142857142}},\n",
       " {'rouge-1': {'f': 0.29292928813335384,\n",
       "   'p': 0.24369747899159663,\n",
       "   'r': 0.3670886075949367},\n",
       "  'rouge-2': {'f': 0.040609132261975885,\n",
       "   'p': 0.03375527426160337,\n",
       "   'r': 0.050955414012738856},\n",
       "  'rouge-l': {'f': 0.23846153349112437,\n",
       "   'p': 0.22142857142857142,\n",
       "   'r': 0.25833333333333336}},\n",
       " {'rouge-1': {'f': 0.18666666171666682,\n",
       "   'p': 0.2074074074074074,\n",
       "   'r': 0.1696969696969697},\n",
       "  'rouge-2': {'f': 0.0067114044466501975,\n",
       "   'p': 0.007462686567164179,\n",
       "   'r': 0.006097560975609756},\n",
       "  'rouge-l': {'f': 0.12682926362688896,\n",
       "   'p': 0.17105263157894737,\n",
       "   'r': 0.10077519379844961}},\n",
       " {'rouge-1': {'f': 0.22981365961961356,\n",
       "   'p': 0.21511627906976744,\n",
       "   'r': 0.24666666666666667},\n",
       "  'rouge-2': {'f': 0.03749999502363347,\n",
       "   'p': 0.03508771929824561,\n",
       "   'r': 0.040268456375838924},\n",
       "  'rouge-l': {'f': 0.1759259209413582,\n",
       "   'p': 0.18627450980392157,\n",
       "   'r': 0.16666666666666666}},\n",
       " {'rouge-1': {'f': 0.206896546758383,\n",
       "   'p': 0.1910828025477707,\n",
       "   'r': 0.22556390977443608},\n",
       "  'rouge-2': {'f': 0.013888883923612885,\n",
       "   'p': 0.01282051282051282,\n",
       "   'r': 0.015151515151515152},\n",
       "  'rouge-l': {'f': 0.18994412907836847,\n",
       "   'p': 0.18888888888888888,\n",
       "   'r': 0.19101123595505617}},\n",
       " {'rouge-1': {'f': 0.2228412208449656,\n",
       "   'p': 0.18433179723502305,\n",
       "   'r': 0.28169014084507044},\n",
       "  'rouge-2': {'f': 0.022408958806111694,\n",
       "   'p': 0.018518518518518517,\n",
       "   'r': 0.028368794326241134},\n",
       "  'rouge-l': {'f': 0.16460904857931563,\n",
       "   'p': 0.145985401459854,\n",
       "   'r': 0.18867924528301888}},\n",
       " {'rouge-1': {'f': 0.23703703226776418,\n",
       "   'p': 0.1951219512195122,\n",
       "   'r': 0.3018867924528302},\n",
       "  'rouge-2': {'f': 0.014925368368513442,\n",
       "   'p': 0.012269938650306749,\n",
       "   'r': 0.01904761904761905},\n",
       "  'rouge-l': {'f': 0.13471502120325393,\n",
       "   'p': 0.10833333333333334,\n",
       "   'r': 0.1780821917808219}},\n",
       " {'rouge-1': {'f': 0.2873900244204987,\n",
       "   'p': 0.3333333333333333,\n",
       "   'r': 0.25257731958762886},\n",
       "  'rouge-2': {'f': 0.023598815155107568,\n",
       "   'p': 0.0273972602739726,\n",
       "   'r': 0.02072538860103627},\n",
       "  'rouge-l': {'f': 0.19230768747457483,\n",
       "   'p': 0.23529411764705882,\n",
       "   'r': 0.16260162601626016}},\n",
       " {'rouge-1': {'f': 0.3038673983822229, 'p': 0.34375, 'r': 0.2722772277227723},\n",
       "  'rouge-2': {'f': 0.04999999506805604,\n",
       "   'p': 0.05660377358490566,\n",
       "   'r': 0.04477611940298507},\n",
       "  'rouge-l': {'f': 0.16736401181001748,\n",
       "   'p': 0.19047619047619047,\n",
       "   'r': 0.14925373134328357}},\n",
       " {'rouge-1': {'f': 0.29295774147923037,\n",
       "   'p': 0.29545454545454547,\n",
       "   'r': 0.2905027932960894},\n",
       "  'rouge-2': {'f': 0.039660051657585564, 'p': 0.04, 'r': 0.03932584269662921},\n",
       "  'rouge-l': {'f': 0.18181817682277332,\n",
       "   'p': 0.1875,\n",
       "   'r': 0.17647058823529413}},\n",
       " {'rouge-1': {'f': 0.26865671143028647,\n",
       "   'p': 0.2559241706161137,\n",
       "   'r': 0.28272251308900526},\n",
       "  'rouge-2': {'f': 0.019999995012501246,\n",
       "   'p': 0.01904761904761905,\n",
       "   'r': 0.021052631578947368},\n",
       "  'rouge-l': {'f': 0.1706161088663778,\n",
       "   'p': 0.20224719101123595,\n",
       "   'r': 0.14754098360655737}},\n",
       " {'rouge-1': {'f': 0.23648648149767176,\n",
       "   'p': 0.24822695035460993,\n",
       "   'r': 0.22580645161290322},\n",
       "  'rouge-2': {'f': 0.02721087936508028,\n",
       "   'p': 0.02857142857142857,\n",
       "   'r': 0.025974025974025976},\n",
       "  'rouge-l': {'f': 0.15228425896982675,\n",
       "   'p': 0.1595744680851064,\n",
       "   'r': 0.14563106796116504}},\n",
       " {'rouge-1': {'f': 0.23923444477221226,\n",
       "   'p': 0.228310502283105,\n",
       "   'r': 0.25125628140703515},\n",
       "  'rouge-2': {'f': 0.01923076424232748,\n",
       "   'p': 0.01834862385321101,\n",
       "   'r': 0.020202020202020204},\n",
       "  'rouge-l': {'f': 0.14345991063184335,\n",
       "   'p': 0.15315315315315314,\n",
       "   'r': 0.1349206349206349}},\n",
       " {'rouge-1': {'f': 0.21518986875440643,\n",
       "   'p': 0.1708542713567839,\n",
       "   'r': 0.2905982905982906},\n",
       "  'rouge-2': {'f': 0.04458598260213445,\n",
       "   'p': 0.03535353535353535,\n",
       "   'r': 0.0603448275862069},\n",
       "  'rouge-l': {'f': 0.17435896938119674,\n",
       "   'p': 0.16346153846153846,\n",
       "   'r': 0.18681318681318682}},\n",
       " {'rouge-1': {'f': 0.24128685830531385,\n",
       "   'p': 0.22277227722772278,\n",
       "   'r': 0.2631578947368421},\n",
       "  'rouge-2': {'f': 0.03234500851199937,\n",
       "   'p': 0.029850746268656716,\n",
       "   'r': 0.03529411764705882},\n",
       "  'rouge-l': {'f': 0.16814158794541484,\n",
       "   'p': 0.18095238095238095,\n",
       "   'r': 0.15702479338842976}},\n",
       " {'rouge-1': {'f': 0.3230240500088568,\n",
       "   'p': 0.30128205128205127,\n",
       "   'r': 0.34814814814814815},\n",
       "  'rouge-2': {'f': 0.1107266386250167,\n",
       "   'p': 0.1032258064516129,\n",
       "   'r': 0.11940298507462686},\n",
       "  'rouge-l': {'f': 0.2499999950005426,\n",
       "   'p': 0.24742268041237114,\n",
       "   'r': 0.25263157894736843}},\n",
       " {'rouge-1': {'f': 0.2027026977211926,\n",
       "   'p': 0.1910828025477707,\n",
       "   'r': 0.2158273381294964},\n",
       "  'rouge-2': {'f': 0.020408158284049526,\n",
       "   'p': 0.019230769230769232,\n",
       "   'r': 0.021739130434782608},\n",
       "  'rouge-l': {'f': 0.18652849248033518,\n",
       "   'p': 0.16666666666666666,\n",
       "   'r': 0.21176470588235294}},\n",
       " {'rouge-1': {'f': 0.3506849265429162,\n",
       "   'p': 0.32323232323232326,\n",
       "   'r': 0.38323353293413176},\n",
       "  'rouge-2': {'f': 0.05509641376924815,\n",
       "   'p': 0.050761421319796954,\n",
       "   'r': 0.060240963855421686},\n",
       "  'rouge-l': {'f': 0.20833332841145846,\n",
       "   'p': 0.18518518518518517,\n",
       "   'r': 0.23809523809523808}},\n",
       " {'rouge-1': {'f': 0.33333332840577856,\n",
       "   'p': 0.37894736842105264,\n",
       "   'r': 0.2975206611570248},\n",
       "  'rouge-2': {'f': 0.07906976251498138,\n",
       "   'p': 0.08994708994708994,\n",
       "   'r': 0.07053941908713693},\n",
       "  'rouge-l': {'f': 0.17120622076337277,\n",
       "   'p': 0.19642857142857142,\n",
       "   'r': 0.15172413793103448}},\n",
       " {'rouge-1': {'f': 0.35148514351926285,\n",
       "   'p': 0.34134615384615385,\n",
       "   'r': 0.3622448979591837},\n",
       "  'rouge-2': {'f': 0.05970148754176918,\n",
       "   'p': 0.057971014492753624,\n",
       "   'r': 0.06153846153846154},\n",
       "  'rouge-l': {'f': 0.23868312260258442,\n",
       "   'p': 0.25892857142857145,\n",
       "   'r': 0.22137404580152673}},\n",
       " {'rouge-1': {'f': 0.32028469261534176, 'p': 0.375, 'r': 0.2795031055900621},\n",
       "  'rouge-2': {'f': 0.05017920657751105,\n",
       "   'p': 0.058823529411764705,\n",
       "   'r': 0.04375},\n",
       "  'rouge-l': {'f': 0.25581394862087625, 'p': 0.3055555555555556, 'r': 0.22}},\n",
       " {'rouge-1': {'f': 0.16603773226457824,\n",
       "   'p': 0.10837438423645321,\n",
       "   'r': 0.3548387096774194},\n",
       "  'rouge-2': {'f': 0.015209121912418248,\n",
       "   'p': 0.009900990099009901,\n",
       "   'r': 0.03278688524590164},\n",
       "  'rouge-l': {'f': 0.15909090492768604,\n",
       "   'p': 0.11290322580645161,\n",
       "   'r': 0.2692307692307692}},\n",
       " {'rouge-1': {'f': 0.19191918711317738,\n",
       "   'p': 0.16033755274261605,\n",
       "   'r': 0.2389937106918239},\n",
       "  'rouge-2': {'f': 0.03553299011981822,\n",
       "   'p': 0.029661016949152543,\n",
       "   'r': 0.04430379746835443},\n",
       "  'rouge-l': {'f': 0.1228070125534782,\n",
       "   'p': 0.11764705882352941,\n",
       "   'r': 0.12844036697247707}},\n",
       " {'rouge-1': {'f': 0.15012106173032624,\n",
       "   'p': 0.09872611464968153,\n",
       "   'r': 0.31313131313131315},\n",
       "  'rouge-2': {'f': 0.019464716562891022,\n",
       "   'p': 0.012779552715654952,\n",
       "   'r': 0.04081632653061224},\n",
       "  'rouge-l': {'f': 0.12875536041113314,\n",
       "   'p': 0.09554140127388536,\n",
       "   'r': 0.19736842105263158}},\n",
       " {'rouge-1': {'f': 0.2512562765706675,\n",
       "   'p': 0.2127659574468085,\n",
       "   'r': 0.3067484662576687},\n",
       "  'rouge-2': {'f': 0.005050500215798935,\n",
       "   'p': 0.004273504273504274,\n",
       "   'r': 0.006172839506172839},\n",
       "  'rouge-l': {'f': 0.15328466660983553, 'p': 0.13636363636363635, 'r': 0.175}},\n",
       " {'rouge-1': {'f': 0.16417909949543344,\n",
       "   'p': 0.1746031746031746,\n",
       "   'r': 0.15492957746478872},\n",
       "  'rouge-2': {'f': 0.015037589003054394,\n",
       "   'p': 0.016,\n",
       "   'r': 0.014184397163120567},\n",
       "  'rouge-l': {'f': 0.16304347331994345,\n",
       "   'p': 0.18292682926829268,\n",
       "   'r': 0.14705882352941177}},\n",
       " {'rouge-1': {'f': 0.2413793054135554,\n",
       "   'p': 0.21604938271604937,\n",
       "   'r': 0.2734375},\n",
       "  'rouge-2': {'f': 0.013888883958576207,\n",
       "   'p': 0.012422360248447204,\n",
       "   'r': 0.015748031496062992},\n",
       "  'rouge-l': {'f': 0.15228425897498019,\n",
       "   'p': 0.14423076923076922,\n",
       "   'r': 0.16129032258064516}},\n",
       " {'rouge-1': {'f': 0.2436548176701281,\n",
       "   'p': 0.1935483870967742,\n",
       "   'r': 0.3287671232876712},\n",
       "  'rouge-2': {'f': 0.03061224023649071,\n",
       "   'p': 0.024291497975708502,\n",
       "   'r': 0.041379310344827586},\n",
       "  'rouge-l': {'f': 0.1718749951477052,\n",
       "   'p': 0.14666666666666667,\n",
       "   'r': 0.20754716981132076}},\n",
       " {'rouge-1': {'f': 0.2517006758276644,\n",
       "   'p': 0.18877551020408162,\n",
       "   'r': 0.37755102040816324},\n",
       "  'rouge-2': {'f': 0.027397255837165287,\n",
       "   'p': 0.020512820512820513,\n",
       "   'r': 0.041237113402061855},\n",
       "  'rouge-l': {'f': 0.19095476925128166,\n",
       "   'p': 0.14960629921259844,\n",
       "   'r': 0.2638888888888889}},\n",
       " {'rouge-1': {'f': 0.20467835760661413,\n",
       "   'p': 0.1891891891891892,\n",
       "   'r': 0.2229299363057325},\n",
       "  'rouge-2': {'f': 0.01764705385744084,\n",
       "   'p': 0.016304347826086956,\n",
       "   'r': 0.019230769230769232},\n",
       "  'rouge-l': {'f': 0.1808510590793347,\n",
       "   'p': 0.2328767123287671,\n",
       "   'r': 0.14782608695652175}},\n",
       " {'rouge-1': {'f': 0.3358024641382717,\n",
       "   'p': 0.3434343434343434,\n",
       "   'r': 0.3285024154589372},\n",
       "  'rouge-2': {'f': 0.03473944909678723,\n",
       "   'p': 0.03553299492385787,\n",
       "   'r': 0.03398058252427184},\n",
       "  'rouge-l': {'f': 0.2283464517040735,\n",
       "   'p': 0.2396694214876033,\n",
       "   'r': 0.21804511278195488}}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(rouge_scores):\n",
    "    rouge_1_precision = np.mean([score['rouge-1']['p'] for score in rouge_scores])\n",
    "    rouge_1_recall = np.mean([score['rouge-1']['r'] for score in rouge_scores])\n",
    "    rouge_1_f1 = np.mean([score['rouge-1']['f'] for score in rouge_scores])\n",
    "    \n",
    "    rouge_2_precision = np.mean([score['rouge-2']['p'] for score in rouge_scores])\n",
    "    rouge_2_recall = np.mean([score['rouge-2']['r'] for score in rouge_scores])\n",
    "    rouge_2_f1 = np.mean([score['rouge-2']['f'] for score in rouge_scores])\n",
    "    \n",
    "    rouge_l_precision = np.mean([score['rouge-l']['p'] for score in rouge_scores])\n",
    "    rouge_l_recall = np.mean([score['rouge-l']['r'] for score in rouge_scores])\n",
    "    rouge_l_f1 = np.mean([score['rouge-l']['f'] for score in rouge_scores])\n",
    "    \n",
    "    return rouge_1_precision, rouge_1_recall, rouge_1_f1, rouge_2_precision, rouge_2_recall, rouge_2_f1, rouge_l_precision, rouge_l_recall, rouge_l_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 Precision: 0.2281, Recall: 0.2965, F1: 0.2464\n",
      "ROUGE-2 Precision: 0.0321, Recall: 0.0415, F1: 0.0343\n",
      "ROUGE-L Precision: 0.1768, Recall: 0.2003, F1: 0.1808\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rouge_1_precision, rouge_1_recall, rouge_1_f1, rouge_2_precision, rouge_2_recall, rouge_2_f1, rouge_l_precision, rouge_l_recall, rouge_l_f1 = compute_metrics(rouge_scores)\n",
    "\n",
    "print(f\"ROUGE-1 Precision: {rouge_1_precision:.4f}, Recall: {rouge_1_recall:.4f}, F1: {rouge_1_f1:.4f}\")\n",
    "print(f\"ROUGE-2 Precision: {rouge_2_precision:.4f}, Recall: {rouge_2_recall:.4f}, F1: {rouge_2_f1:.4f}\")\n",
    "print(f\"ROUGE-L Precision: {rouge_l_precision:.4f}, Recall: {rouge_l_recall:.4f}, F1: {rouge_l_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an unsupervised extractive summarization algorithm, I will then use training data to experiment with different hyperparameters and preprocessing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment and Adjust the number of sentences using training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different numbers of sentences on the validation dataset\n",
    "min_sentences = 2\n",
    "max_sentences = 7\n",
    "best_num_sentences = 0\n",
    "best_avg_rouge_1_f1 = 0\n",
    "\n",
    "for num_sentences in range(min_sentences, max_sentences + 1):\n",
    "    rouge_scores = evaluate(train_df, num_sentences=num_sentences)\n",
    "    _, _, rouge_1_f1, _, _, _, _, _, _ = compute_metrics(rouge_scores)\n",
    "    avg_rouge_1_f1 = np.mean(rouge_1_f1)\n",
    "\n",
    "    print(f\"Number of sentences: {num_sentences}, Avg. ROUGE-1 F1: {avg_rouge_1_f1:.4f}\")\n",
    "\n",
    "    if avg_rouge_1_f1 > best_avg_rouge_1_f1:\n",
    "        best_num_sentences = num_sentences\n",
    "        best_avg_rouge_1_f1 = avg_rouge_1_f1\n",
    "\n",
    "print(f\"\\nThe best number of sentences is {best_num_sentences} with an average ROUGE-1 F1 score of {best_avg_rouge_1_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_scores_test = evaluate(test_df, num_sentences=best_num_sentences)\n",
    "rouge_1_precision, rouge_1_recall, rouge_1_f1, rouge_2_precision, rouge_2_recall, rouge_2_f1, rouge_l_precision, rouge_l_recall, rouge_l_f1 = compute_metrics(rouge_scores_test)\n",
    "\n",
    "print(f\"\\nTest dataset performance using {best_num_sentences} sentences:\")\n",
    "print(f\"ROUGE-1 Precision: {rouge_1_precision:.4f}, Recall: {rouge_1_recall:.4f}, F1: {rouge_1_f1:.4f}\")\n",
    "print(f\"ROUGE-2 Precision: {rouge_2_precision:.4f}, Recall: {rouge_2_recall:.4f}, F1: {rouge_2_f1:.4f}\")\n",
    "print(f\"ROUGE-L Precision: {rouge_l_precision:.4f}, Recall: {rouge_l_recall:.4f}, F1: {rouge_l_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, 3 is the best sentense length for textrank model; the performance is what have been calculated in the original model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: It is hard to directly use textrake to input the whole paper and generate summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary-based pre-processing: Feature engineering, select only top n keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to analyze the training data to identify the most relevant words, phrases, or entities that contribute to a good summary. Use this information to pre-process the input text and filter out irrelevant content before applying TextRank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def top_n_keywords(document, n=10):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform([document])\n",
    "    feature_array = np.array(vectorizer.get_feature_names())\n",
    "    tfidf_sorting = np.argsort(tfidf_matrix.toarray()).flatten()[::-1]\n",
    "    top_n = feature_array[tfidf_sorting][:n]\n",
    "    return top_n\n",
    "\n",
    "def preprocess_text(text, top_n):\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [token for token in tokens if token.lower() in top_n]\n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model to identify top_n keywords based on training data\n",
    "train_documents = train_df['body'].apply(str).tolist()\n",
    "combined_train_documents = ' '.join(train_documents)\n",
    "top_n = 50  # You can experiment with different values for top_n\n",
    "top_n_keywords = top_n_keywords(combined_train_documents, n=top_n)\n",
    "\n",
    "# Modify the extractive_summarizer to use preprocessed text\n",
    "def extractive_summarizer(text, num_sentences=3, top_n_keywords=None):\n",
    "    if top_n_keywords:\n",
    "        text = preprocess_text(text, top_n_keywords)\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    summarizer = TextRankSummarizer()\n",
    "    summary = summarizer(parser.document, num_sentences)\n",
    "    \n",
    "    summarized_text = '. '.join([str(sentence) for sentence in summary]) + '.'\n",
    "    return summarized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test dataset performance using 3 sentences:\n",
      "ROUGE-1 Precision: 0.2450, Recall: 0.2865, F1: 0.2536\n",
      "ROUGE-2 Precision: 0.0428, Recall: 0.0473, F1: 0.0432\n",
      "ROUGE-L Precision: 0.1918, Recall: 0.1948, F1: 0.1871\n"
     ]
    }
   ],
   "source": [
    "rouge_scores_test = evaluate(test_df, num_sentences=3)\n",
    "rouge_1_precision, rouge_1_recall, rouge_1_f1, rouge_2_precision, rouge_2_recall, rouge_2_f1, rouge_l_precision, rouge_l_recall, rouge_l_f1 = compute_metrics(rouge_scores_test)\n",
    "\n",
    "print(f\"\\nTest dataset performance using {best_num_sentences} sentences:\")\n",
    "print(f\"ROUGE-1 Precision: {rouge_1_precision:.4f}, Recall: {rouge_1_recall:.4f}, F1: {rouge_1_f1:.4f}\")\n",
    "print(f\"ROUGE-2 Precision: {rouge_2_precision:.4f}, Recall: {rouge_2_recall:.4f}, F1: {rouge_2_f1:.4f}\")\n",
    "print(f\"ROUGE-L Precision: {rouge_l_precision:.4f}, Recall: {rouge_l_recall:.4f}, F1: {rouge_l_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/xinye/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/xinye/nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/xinye/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def preprocess_text(text, top_n, lemmatize=False):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tagged_tokens = nltk.pos_tag(tokens)\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token, pos=get_wordnet_pos(pos)) for token, pos in tagged_tokens]\n",
    "        tokens = lemmatized_tokens\n",
    "\n",
    "    filtered_tokens = [token for token in tokens if token.lower() in top_n]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "def extractive_summarizer(text, num_sentences=3, top_n_keywords=None, lemmatize=False):\n",
    "    if top_n_keywords is not None and len(top_n_keywords) > 0:\n",
    "        text = preprocess_text(text, top_n_keywords, lemmatize=lemmatize)\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    summarizer = TextRankSummarizer()\n",
    "    summary = summarizer(parser.document, num_sentences)\n",
    "    \n",
    "    summarized_text = '. '.join([str(sentence) for sentence in summary]) + '.'\n",
    "    return summarized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test dataset performance using 3 sentences and lemmatization:\n",
      "ROUGE-1 Precision: 0.0375, Recall: 0.0659, F1: 0.0425\n",
      "ROUGE-2 Precision: 0.0021, Recall: 0.0031, F1: 0.0020\n",
      "ROUGE-L Precision: 0.1893, Recall: 0.0619, F1: 0.0914\n"
     ]
    }
   ],
   "source": [
    "def evaluate(df, num_sentences=3, lemmatize=False):\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        text = row['body']\n",
    "        reference = row['summary']\n",
    "        summary = extractive_summarizer(text, num_sentences, top_n_keywords=top_n_keywords, lemmatize=lemmatize)\n",
    "        \n",
    "        score = rouge.get_scores(summary, reference, avg=True)\n",
    "        rouge_scores.append(score)\n",
    "    \n",
    "    return rouge_scores\n",
    "\n",
    "rouge_scores_lemmatized = evaluate(test_df, num_sentences=best_num_sentences, lemmatize=True)\n",
    "rouge_1_precision, rouge_1_recall, rouge_1_f1, rouge_2_precision, rouge_2_recall, rouge_2_f1, rouge_l_precision, rouge_l_recall, rouge_l_f1 = compute_metrics(rouge_scores_lemmatized)\n",
    "\n",
    "print(f\"\\nTest dataset performance using {best_num_sentences} sentences and lemmatization:\")\n",
    "print(f\"ROUGE-1 Precision: {rouge_1_precision:.4f}, Recall: {rouge_1_recall:.4f}, F1: {rouge_1_f1:.4f}\")\n",
    "print(f\"ROUGE-2 Precision: {rouge_2_precision:.4f}, Recall: {rouge_2_recall:.4f}, F1: {rouge_2_f1:.4f}\")\n",
    "print(f\"ROUGE-L Precision: {rouge_l_precision:.4f}, Recall: {rouge_l_recall:.4f}, F1: {rouge_l_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is will reduce the performance of the textrank model, indicating it might not for summarization task, or for this special clean dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Model 2: LexRank and Sentence-transfomers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/sentence-transformers\n",
    "\n",
    "https://github.com/UKPLab/sentence-transformers\n",
    "\n",
    "https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/text-summarization\n",
    "\n",
    "Here we used sentence-transformers and lexrank to generate extractive summarization for the paper input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree_centrality_scores(similarity_matrix):\n",
    "    row_sums = np.sum(similarity_matrix, axis=1)\n",
    "    row_sums[row_sums == 0] = 1 # avoid divide by 0\n",
    "    normalized_similarity_matrix = similarity_matrix / row_sums[:, np.newaxis] # Normalization\n",
    "    \n",
    "    # eigenvector\n",
    "    eigvals, eigvecs = np.linalg.eig(normalized_similarity_matrix.T)\n",
    "    eigvec = np.real(eigvecs[:, np.argmax(eigvals)])\n",
    "    centrality_scores = eigvec / np.sum(eigvec)\n",
    "    \n",
    "    return centrality_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexrank_summarize(document, num_sentences=8):\n",
    "    # The code below are adpated based on the code example provided by sententransformer author Nils Reimers:\n",
    "    # https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/text-summarization/text-summarization.py\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "    \n",
    "    scores = util.cos_sim(embeddings, embeddings)\n",
    "    scores_np = scores.cpu().numpy()\n",
    "    centrality_scores = degree_centrality_scores(scores_np)\n",
    "    most_central_sentence_indices = np.argsort(-centrality_scores)\n",
    "\n",
    "    top_indices = most_central_sentence_indices[:num_sentences]\n",
    "    top_indices.sort()\n",
    "    summary = \". \".join([sentences[i] for i in top_indices])\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 Precision: 0.2829, Recall: 0.4166, F1: 0.3233\n",
      "ROUGE-2 Precision: 0.0701, Recall: 0.1037, F1: 0.0797\n",
      "ROUGE-L Precision: 0.2374, Recall: 0.3056, F1: 0.2595\n"
     ]
    }
   ],
   "source": [
    "def evaluate(df):\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        text = row['body']\n",
    "        reference = row['summary']\n",
    "        summary =lexrank_summarize(text)\n",
    "        \n",
    "        score = rouge.get_scores(summary, reference, avg=True)\n",
    "        rouge_scores.append(score)\n",
    "    \n",
    "    return rouge_scores\n",
    "\n",
    "rouge_scores = evaluate(test_df)\n",
    "rouge_1_precision, rouge_1_recall, rouge_1_f1, rouge_2_precision, rouge_2_recall, rouge_2_f1, rouge_l_precision, rouge_l_recall, rouge_l_f1 = compute_metrics(rouge_scores)\n",
    "\n",
    "print(f\"ROUGE-1 Precision: {rouge_1_precision:.4f}, Recall: {rouge_1_recall:.4f}, F1: {rouge_1_f1:.4f}\")\n",
    "print(f\"ROUGE-2 Precision: {rouge_2_precision:.4f}, Recall: {rouge_2_recall:.4f}, F1: {rouge_2_f1:.4f}\")\n",
    "print(f\"ROUGE-L Precision: {rouge_l_precision:.4f}, Recall: {rouge_l_recall:.4f}, F1: {rouge_l_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive model3: clsutering and sentence-transformers\n",
    "https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L3-v2\n",
    "\n",
    "@inproceedings{reimers-2019-sentence-bert,\n",
    "  title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n",
    "  author = \"Reimers, Nils and Gurevych, Iryna\",\n",
    "  booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n",
    "  month = \"11\",\n",
    "  year = \"2019\",\n",
    "  publisher = \"Association for Computational Linguistics\",\n",
    "  url = \"https://arxiv.org/abs/1908.10084\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strass_summarization(text, num_clusters=5):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    model = SentenceTransformer('paraphrase-MiniLM-L3-v2', device='cpu')\n",
    "    sentence_embeddings = model.encode(sentences, batch_size=16)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(sentence_embeddings)\n",
    "\n",
    "    closest_sentences_indices = []\n",
    "    for i in range(num_clusters):\n",
    "        centroid = kmeans.cluster_centers_[i]\n",
    "        closest_sentence_idx = np.argmin(np.linalg.norm(sentence_embeddings - centroid, axis=1))\n",
    "        closest_sentences_indices.append(closest_sentence_idx)\n",
    "\n",
    "    important_sentences = [sentences[idx] for idx in sorted(closest_sentences_indices)]\n",
    "    return ' '.join(important_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 Precision: 0.2742, Recall: 0.3416, F1: 0.2948\n",
      "ROUGE-2 Precision: 0.0613, Recall: 0.0749, F1: 0.0655\n",
      "ROUGE-L Precision: 0.2146, Recall: 0.2522, F1: 0.2266\n"
     ]
    }
   ],
   "source": [
    "def evaluate(df):\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        text = row['body']\n",
    "        reference = row['summary']\n",
    "        summary = strass_summarization(text)\n",
    "        \n",
    "        score = rouge.get_scores(summary, reference, avg=True)\n",
    "        rouge_scores.append(score)\n",
    "    \n",
    "    return rouge_scores\n",
    "\n",
    "rouge_scores_strass= evaluate(test_df)\n",
    "rouge_1_precision, rouge_1_recall, rouge_1_f1, rouge_2_precision, rouge_2_recall, rouge_2_f1, rouge_l_precision, rouge_l_recall, rouge_l_f1 = compute_metrics(rouge_scores_strass)\n",
    "\n",
    "print(f\"ROUGE-1 Precision: {rouge_1_precision:.4f}, Recall: {rouge_1_recall:.4f}, F1: {rouge_1_f1:.4f}\")\n",
    "print(f\"ROUGE-2 Precision: {rouge_2_precision:.4f}, Recall: {rouge_2_recall:.4f}, F1: {rouge_2_f1:.4f}\")\n",
    "print(f\"ROUGE-L Precision: {rouge_l_precision:.4f}, Recall: {rouge_l_recall:.4f}, F1: {rouge_l_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experiment with clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strass_summarization(text, num_clusters):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    model = SentenceTransformer('paraphrase-MiniLM-L3-v2', device='cpu')\n",
    "    sentence_embeddings = model.encode(sentences, batch_size=16)\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(sentence_embeddings)\n",
    "\n",
    "    closest_sentences_indices = []\n",
    "    for i in range(num_clusters):\n",
    "        centroid = kmeans.cluster_centers_[i]\n",
    "        closest_sentence_idx = np.argmin(np.linalg.norm(sentence_embeddings - centroid, axis=1))\n",
    "        closest_sentences_indices.append(closest_sentence_idx)\n",
    "\n",
    "    important_sentences = [sentences[idx] for idx in sorted(closest_sentences_indices)]\n",
    "    return ' '.join(important_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(df, num_clusters):\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        text = row['body']\n",
    "        reference = row['summary']\n",
    "        summary = strass_summarization(text, num_clusters=num_clusters)\n",
    "        \n",
    "        score = rouge.get_scores(summary, reference, avg=True)\n",
    "        rouge_scores.append(score)\n",
    "    \n",
    "    return rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 2, Avg. ROUGE-1 F1: 0.2383\n",
      "Number of clusters: 3, Avg. ROUGE-1 F1: 0.2601\n",
      "Number of clusters: 4, Avg. ROUGE-1 F1: 0.2678\n",
      "Number of clusters: 5, Avg. ROUGE-1 F1: 0.2740\n",
      "Number of clusters: 6, Avg. ROUGE-1 F1: 0.2774\n",
      "Number of clusters: 7, Avg. ROUGE-1 F1: 0.2721\n",
      "Number of clusters: 8, Avg. ROUGE-1 F1: 0.2738\n",
      "Number of clusters: 9, Avg. ROUGE-1 F1: 0.2730\n",
      "Number of clusters: 10, Avg. ROUGE-1 F1: 0.2708\n",
      "\n",
      "The best number of clusters is 6 with an average ROUGE-1 F1 score of 0.2774\n"
     ]
    }
   ],
   "source": [
    "\n",
    "min_cluster = 2\n",
    "max_cluster = 10\n",
    "best_num_cluster = 0\n",
    "best_avg_rouge_1_f1 = 0\n",
    "\n",
    "for num in range(min_cluster, max_cluster + 1):\n",
    "    rouge_scores = evaluate(val_df, num_clusters=num)\n",
    "    _, _, rouge_1_f1, _, _, _, _, _, _ = compute_metrics(rouge_scores)\n",
    "    avg_rouge_1_f1 = np.mean(rouge_1_f1)\n",
    "\n",
    "    print(f\"Number of clusters: {num}, Avg. ROUGE-1 F1: {avg_rouge_1_f1:.4f}\")\n",
    "\n",
    "    if avg_rouge_1_f1 > best_avg_rouge_1_f1:\n",
    "        best_num_cluster = num\n",
    "        best_avg_rouge_1_f1 = avg_rouge_1_f1\n",
    "\n",
    "print(f\"\\nThe best number of clusters is {best_num_cluster} with an average ROUGE-1 F1 score of {best_avg_rouge_1_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 Precision: 0.2564, Recall: 0.3618, F1: 0.2909\n",
      "ROUGE-2 Precision: 0.0542, Recall: 0.0741, F1: 0.0608\n",
      "ROUGE-L Precision: 0.2006, Recall: 0.2632, F1: 0.2226\n"
     ]
    }
   ],
   "source": [
    "rouge_scores_strass= evaluate(test_df, num_clusters=6)\n",
    "rouge_1_precision, rouge_1_recall, rouge_1_f1, rouge_2_precision, rouge_2_recall, rouge_2_f1, rouge_l_precision, rouge_l_recall, rouge_l_f1 = compute_metrics(rouge_scores_strass)\n",
    "\n",
    "print(f\"ROUGE-1 Precision: {rouge_1_precision:.4f}, Recall: {rouge_1_recall:.4f}, F1: {rouge_1_f1:.4f}\")\n",
    "print(f\"ROUGE-2 Precision: {rouge_2_precision:.4f}, Recall: {rouge_2_recall:.4f}, F1: {rouge_2_f1:.4f}\")\n",
    "print(f\"ROUGE-L Precision: {rouge_l_precision:.4f}, Recall: {rouge_l_recall:.4f}, F1: {rouge_l_f1:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Model 4: lexrank+clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "def lexrank_summarize(document, num_sentences=10):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "    \n",
    "    scores = util.cos_sim(embeddings, embeddings)\n",
    "    scores_np = scores.cpu().numpy()\n",
    "    \n",
    "    # cluster sentence embeddings\n",
    "    num_clusters = min(len(sentences), 9)\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(embeddings)\n",
    "    \n",
    "    # get most central sentence from each cluster\n",
    "    most_central_sentence_indices = []\n",
    "    for i in range(num_clusters):\n",
    "        cluster_embeddings = embeddings[kmeans.labels_ == i]\n",
    "        cluster_scores = util.cos_sim(cluster_embeddings, cluster_embeddings).cpu().numpy()\n",
    "        cluster_centrality_scores = degree_centrality_scores(cluster_scores)\n",
    "        most_central_sentence_index = np.argmax(cluster_centrality_scores)\n",
    "        most_central_sentence_indices.append(np.where(kmeans.labels_ == i)[0][most_central_sentence_index])\n",
    "        \n",
    "    top_indices = np.argsort(most_central_sentence_indices)[:num_sentences]\n",
    "    summary = \". \".join([sentences[i] for i in top_indices])\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 Precision: 0.3118, Recall: 0.4365, F1: 0.3508\n",
      "ROUGE-2 Precision: 0.1302, Recall: 0.1694, F1: 0.1411\n",
      "ROUGE-L Precision: 0.2754, Recall: 0.3564, F1: 0.3023\n"
     ]
    }
   ],
   "source": [
    "def evaluate(df):\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        text = row['body']\n",
    "        reference = row['summary']\n",
    "        summary = lexrank_summarize(text)\n",
    "        \n",
    "        score = rouge.get_scores(summary, reference, avg=True)\n",
    "        rouge_scores.append(score)\n",
    "    \n",
    "    return rouge_scores\n",
    "\n",
    "rouge_scores = evaluate(test_df)\n",
    "rouge_1_precision, rouge_1_recall, rouge_1_f1, rouge_2_precision, rouge_2_recall, rouge_2_f1, rouge_l_precision, rouge_l_recall, rouge_l_f1 = compute_metrics(rouge_scores)\n",
    "\n",
    "print(f\"ROUGE-1 Precision: {rouge_1_precision:.4f}, Recall: {rouge_1_recall:.4f}, F1: {rouge_1_f1:.4f}\")\n",
    "print(f\"ROUGE-2 Precision: {rouge_2_precision:.4f}, Recall: {rouge_2_recall:.4f}, F1: {rouge_2_f1:.4f}\")\n",
    "print(f\"ROUGE-L Precision: {rouge_l_precision:.4f}, Recall: {rouge_l_recall:.4f}, F1: {rouge_l_f1:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive+Abstractive: lexrank+clustering+gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "def lexrank_summarize(document, num_sentences=20):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    \n",
    "    if not sentences:  # exclude empty sentences list\n",
    "        return ''\n",
    "    \n",
    "    embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "    \n",
    "    scores = util.cos_sim(embeddings, embeddings)\n",
    "    scores_np = scores.cpu().numpy()\n",
    "    \n",
    "    num_clusters = min(len(sentences), 9)\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(embeddings)\n",
    "    \n",
    "    most_central_sentence_indices = []\n",
    "    for i in range(num_clusters):\n",
    "        cluster_embeddings = embeddings[kmeans.labels_ == i]\n",
    "        cluster_scores = util.cos_sim(cluster_embeddings, cluster_embeddings).cpu().numpy()\n",
    "        cluster_centrality_scores = degree_centrality_scores(cluster_scores)\n",
    "        most_central_sentence_index = np.argmax(cluster_centrality_scores)\n",
    "        most_central_sentence_indices.append(np.where(kmeans.labels_ == i)[0][most_central_sentence_index])\n",
    "        \n",
    "    top_indices = np.argsort(most_central_sentence_indices)[:num_sentences]\n",
    "    summary = \". \".join([sentences[i] for i in top_indices])\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_641872/1298571121.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['body'] = test_df['body'].fillna('')\n",
      "/tmp/ipykernel_641872/1298571121.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['body'] = train_df['body'].fillna('')\n",
      "/tmp/ipykernel_641872/1298571121.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_df['body'] = val_df['body'].fillna('')\n"
     ]
    }
   ],
   "source": [
    "test_df['body'] = test_df['body'].fillna('')\n",
    "train_df['body'] = train_df['body'].fillna('')\n",
    "val_df['body'] = val_df['body'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_641872/3320404106.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['body_extractive'] = train_df['body'].apply(lambda x: lexrank_summarize(x))\n"
     ]
    }
   ],
   "source": [
    "train_df['body_extractive'] = train_df['body'].apply(lambda x: lexrank_summarize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_641872/737083823.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['body_extractive'] = test_df['body'].apply(lambda x: lexrank_summarize(x))\n",
      "/tmp/ipykernel_641872/737083823.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_df['body_extractive'] = val_df['body'].apply(lambda x: lexrank_summarize(x))\n"
     ]
    }
   ],
   "source": [
    "test_df['body_extractive'] = test_df['body'].apply(lambda x: lexrank_summarize(x))\n",
    "val_df['body_extractive'] = val_df['body'].apply(lambda x: lexrank_summarize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('train_df.csv')\n",
    "val_df.to_csv('val_df.csv')\n",
    "test_df.to_csv('test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set the padding token\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Tokenize the data\n",
    "train_encodings = tokenizer(train_df['body_extractive'].tolist(), train_df['summary'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(val_df['body_extractive'].tolist(), val_df['summary'].tolist(), truncation=True, padding=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2SummarizationTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)  #  default for pad_token_id\n",
    "        loss = loss_fct(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "class PaperDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = item[\"input_ids\"].clone()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "train_dataset = PaperDataset(train_encodings)\n",
    "val_dataset = PaperDataset(val_encodings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xinye/.local/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1845' max='1845' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1845/1845 03:51, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.000668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.000665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.000539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.000649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1845, training_loss=0.0032731205467286147, metrics={'train_runtime': 232.0274, 'train_samples_per_second': 15.925, 'train_steps_per_second': 7.952, 'total_flos': 964167598080000.0, 'train_loss': 0.0032731205467286147, 'epoch': 4.99})"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_dir='./logs',\n",
    "    save_strategy='epoch',\n",
    "    fp16=True, \n",
    "    gradient_accumulation_steps=2,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = GPT2SummarizationTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encodings = tokenizer(test_df['body_extractive'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "test_dataset = PaperDataset(test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summaries(model, dataset, tokenizer):\n",
    "    summaries = []\n",
    "    model.eval()\n",
    "\n",
    "    for item in dataset:\n",
    "        with torch.no_grad():\n",
    "            input_ids = item[\"input_ids\"].unsqueeze(0).to(model.device)\n",
    "            attention_mask = item[\"attention_mask\"].unsqueeze(0).to(model.device)\n",
    "            output = model.generate(input_ids, attention_mask=attention_mask)\n",
    "            summary = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            summaries.append(summary)\n",
    "\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "/home/xinye/.local/lib/python3.9/site-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    }
   ],
   "source": [
    "test_summaries = generate_summaries(model, test_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 Precision: 0.3982, Recall: 0.2965, F1: 0.3268\n",
      "ROUGE-2 Precision: 0.1274, Recall: 0.1075, F1: 0.1113\n",
      "ROUGE-L Precision: 0.3200, Recall: 0.2564, F1: 0.2762\n"
     ]
    }
   ],
   "source": [
    "rouge_scores = []\n",
    "\n",
    "for gen_summary, ref_summary in zip(test_summaries, test_df['summary']):\n",
    "    scores = rouge.get_scores(ref_summary, gen_summary)[0]\n",
    "    rouge_scores.append(scores)\n",
    "\n",
    "rouge_1_precision, rouge_1_recall, rouge_1_f1, rouge_2_precision, rouge_2_recall, rouge_2_f1, rouge_l_precision, rouge_l_recall, rouge_l_f1 = compute_metrics(rouge_scores)\n",
    "\n",
    "print(f\"ROUGE-1 Precision: {rouge_1_precision:.4f}, Recall: {rouge_1_recall:.4f}, F1: {rouge_1_f1:.4f}\")\n",
    "print(f\"ROUGE-2 Precision: {rouge_2_precision:.4f}, Recall: {rouge_2_recall:.4f}, F1: {rouge_2_f1:.4f}\")\n",
    "print(f\"ROUGE-L Precision: {rouge_l_precision:.4f}, Recall: {rouge_l_recall:.4f}, F1: {rouge_l_f1:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
