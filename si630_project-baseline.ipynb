{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61qdrCprJdSy"
   },
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "o4qr4fA0FX86"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Jyid84m1KN0P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xinye\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yUUSVmVlKARy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File is unzipped in nlp_data folder\n"
     ]
    }
   ],
   "source": [
    "# url = \"https://cs.stanford.edu/~myasu/projects/scisumm_net/scisummnet_release1.1__20190413.zip\"\n",
    "# response = requests.get(url)\n",
    "# with zipfile.ZipFile(io.BytesIO(response.content)) as zipObj:\n",
    "#      # Extract all the contents of zip file in different directory\n",
    "#      zipObj.extractall(\"nlp_data\")\n",
    "#      print(\"File is unzipped in nlp_data folder\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "WqCmhZy4ItBE"
   },
   "outputs": [],
   "source": [
    "# get all raw text, break all papers into two parts -- Abstract and rest of document\n",
    "# first get all filepaths\n",
    "xmlfiles = []\n",
    "citations = []\n",
    "for subdir, dirs, files in os.walk(r'/home/xinye/nlp_data/scisummnet_release1.1__20190413/top1000_complete'):\n",
    "    for filename in files:\n",
    "        filepath = subdir + os.sep + filename\n",
    "        if filepath.endswith(\".xml\"):\n",
    "            xmlfiles.append(filepath)\n",
    "        if filepath.endswith(\".json\"):\n",
    "            citations.append(filepath)\n",
    "        if filepath.endswith(\".txt\"):\n",
    "            summary.append(filepath)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "ZermgmX3KcdD"
   },
   "outputs": [],
   "source": [
    "#next parse all XML documents\n",
    "\n",
    "def parse_xml_abstract(fp):\n",
    "    \"\"\" parse an XML journal article into an abstract and the rest of the text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(fp)\n",
    "    except Exception as e:\n",
    "        return \"\",\"\",str(e)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    ab = []\n",
    "    bod = []\n",
    "    \n",
    "    for child in root:\n",
    "        if child.tag == \"ABSTRACT\":\n",
    "            for block in child:\n",
    "                ab.append(block.text)\n",
    "        else:\n",
    "            for block in child:\n",
    "                bod.append(block.text)\n",
    "                \n",
    "    #convert from list --> string\n",
    "    abstract = \"\\n\".join(ab)\n",
    "    body = \"\\n\".join(bod)\n",
    "    \n",
    "    #decode html entities\n",
    "    abstract = html.unescape(abstract)\n",
    "    body = html.unescape(body)\n",
    "    \n",
    "    return abstract,body,\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "jhRNHIH2KgfU"
   },
   "outputs": [],
   "source": [
    "raw_cols = []\n",
    "for fpn in range(len(xmlfiles)):\n",
    "    ab,bod,err = parse_xml_abstract(xmlfiles[fpn])\n",
    "    if err:\n",
    "        #print(fp, err)\n",
    "        continue\n",
    "    f = open(citations[fpn]) \n",
    "\n",
    "    # returns JSON object as  \n",
    "    # a dictionary \n",
    "    data = json.load(f) \n",
    "    only_text = []\n",
    "    for entry in data:\n",
    "        only_text.append(entry['clean_text'])\n",
    "#     print(only_text)\n",
    "        \n",
    "    raw_cols.append([ab,bod,only_text,xmlfiles[fpn]])\n",
    "\n",
    "df = pd.DataFrame(raw_cols, columns=[\"abstract\",\"body\",\"citations\", \"filepath\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all summary text using the summary path\n",
    "\n",
    "summary_text = []\n",
    "for fpn in range(len(summary)):\n",
    "    f = open(summary[fpn]) \n",
    "    data = f.read()\n",
    "    summary_text.append(data)\n",
    "\n",
    "df['summary'] = summary_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with missing values or empty abstracts\n",
    "df = df.dropna()\n",
    "df = df[df.abstract != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>body</th>\n",
       "      <th>citations</th>\n",
       "      <th>filepath</th>\n",
       "      <th>summary</th>\n",
       "      <th>paper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2: Test set performance of our systems: and ou...</td>\n",
       "      <td>The open source Moses (Koehn et al., 2007) MT ...</td>\n",
       "      <td>[Factored translation models have also been us...</td>\n",
       "      <td>/home/xinye/nlp_data/scisummnet_release1.1__20...</td>\n",
       "      <td>Experiments in Domain Adaptation for Statistic...</td>\n",
       "      <td>The open source Moses (Koehn et al., 2007) MT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>At present, adapting an Information Extraction...</td>\n",
       "      <td>Most of the world’s information is recorded, p...</td>\n",
       "      <td>[Our work is related to previous work on domai...</td>\n",
       "      <td>/home/xinye/nlp_data/scisummnet_release1.1__20...</td>\n",
       "      <td>On-Demand Information Extraction\\nAt present, ...</td>\n",
       "      <td>Most of the world’s information is recorded, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In this paper, we have proposed novel methods ...</td>\n",
       "      <td>In this paper, we have proposed novel methods ...</td>\n",
       "      <td>[These are much finer grained than Penn Treeba...</td>\n",
       "      <td>/home/xinye/nlp_data/scisummnet_release1.1__20...</td>\n",
       "      <td>Supertagging: An Approach To Almost Parsing\\nI...</td>\n",
       "      <td>In this paper, we have proposed novel methods ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Statistical machine translation systems are us...</td>\n",
       "      <td>In statistical machine translation (SMT), tran...</td>\n",
       "      <td>[Transductive learning method (Ueffing et al, ...</td>\n",
       "      <td>/home/xinye/nlp_data/scisummnet_release1.1__20...</td>\n",
       "      <td>Transductive learning for statistical machine ...</td>\n",
       "      <td>In statistical machine translation (SMT), tran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We combine the strengths of Bayesian modeling ...</td>\n",
       "      <td>Most state-of-the-art statistical machine tran...</td>\n",
       "      <td>[This is in line with earlier work on consiste...</td>\n",
       "      <td>/home/xinye/nlp_data/scisummnet_release1.1__20...</td>\n",
       "      <td>Bayesian Learning of Non-Compositional Phrases...</td>\n",
       "      <td>Most state-of-the-art statistical machine tran...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  \\\n",
       "0  2: Test set performance of our systems: and ou...   \n",
       "1  At present, adapting an Information Extraction...   \n",
       "2  In this paper, we have proposed novel methods ...   \n",
       "3  Statistical machine translation systems are us...   \n",
       "4  We combine the strengths of Bayesian modeling ...   \n",
       "\n",
       "                                                body  \\\n",
       "0  The open source Moses (Koehn et al., 2007) MT ...   \n",
       "1  Most of the world’s information is recorded, p...   \n",
       "2  In this paper, we have proposed novel methods ...   \n",
       "3  In statistical machine translation (SMT), tran...   \n",
       "4  Most state-of-the-art statistical machine tran...   \n",
       "\n",
       "                                           citations  \\\n",
       "0  [Factored translation models have also been us...   \n",
       "1  [Our work is related to previous work on domai...   \n",
       "2  [These are much finer grained than Penn Treeba...   \n",
       "3  [Transductive learning method (Ueffing et al, ...   \n",
       "4  [This is in line with earlier work on consiste...   \n",
       "\n",
       "                                            filepath  \\\n",
       "0  /home/xinye/nlp_data/scisummnet_release1.1__20...   \n",
       "1  /home/xinye/nlp_data/scisummnet_release1.1__20...   \n",
       "2  /home/xinye/nlp_data/scisummnet_release1.1__20...   \n",
       "3  /home/xinye/nlp_data/scisummnet_release1.1__20...   \n",
       "4  /home/xinye/nlp_data/scisummnet_release1.1__20...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Experiments in Domain Adaptation for Statistic...   \n",
       "1  On-Demand Information Extraction\\nAt present, ...   \n",
       "2  Supertagging: An Approach To Almost Parsing\\nI...   \n",
       "3  Transductive learning for statistical machine ...   \n",
       "4  Bayesian Learning of Non-Compositional Phrases...   \n",
       "\n",
       "                                               paper  \n",
       "0  The open source Moses (Koehn et al., 2007) MT ...  \n",
       "1  Most of the world’s information is recorded, p...  \n",
       "2  In this paper, we have proposed novel methods ...  \n",
       "3  In statistical machine translation (SMT), tran...  \n",
       "4  Most state-of-the-art statistical machine tran...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['paper'] = df['body'] + ' ' + df['abstract']\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load NLTK's stop words list\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline: first sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_sentence_summary(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    if sentences:\n",
    "        return sentences[0]\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['first_sentence'] = test_df['body'].apply(lambda x: first_sentence_summary(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1 score: 0.15\n",
      "Average ROUGE-2 score: 0.05\n",
      "Average ROUGE-SU4 score: 0.00\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "rouge = Rouge()\n",
    "\n",
    "test_df['rouge_scores'] = test_df.apply(lambda row: rouge.get_scores(row['first_sentence'], row['summary'])[0], axis=1)\n",
    "\n",
    "avg_rouge_scores = {\n",
    "    'rouge-1': sum([s['rouge-1']['f'] for s in test_df['rouge_scores']]) / len(test_df),\n",
    "    'rouge-2': sum([s['rouge-2']['f'] for s in test_df['rouge_scores']]) / len(test_df),\n",
    "    'rouge-su4': sum([s['rouge-su4']['f'] for s in test_df['rouge_scores'] if 'rouge-su4' in s]) / len(test_df),\n",
    "}\n",
    "\n",
    "print(f\"Average ROUGE-1 score: {avg_rouge_scores['rouge-1']:.2f}\")\n",
    "print(f\"Average ROUGE-2 score: {avg_rouge_scores['rouge-2']:.2f}\")\n",
    "print(f\"Average ROUGE-SU4 score: {avg_rouge_scores['rouge-su4']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1 score: 0.15\n",
      "Average ROUGE-2 score: 0.05\n",
      "Average ROUGE-L score: 0.14\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "test_df['rouge_scores'] = test_df.apply(lambda row: rouge.get_scores(row['first_sentence'], row['summary'])[0], axis=1)\n",
    "\n",
    "avg_rouge_scores = {\n",
    "    'rouge-1': sum([s['rouge-1']['f'] for s in test_df['rouge_scores']]) / len(test_df),\n",
    "    'rouge-2': sum([s['rouge-2']['f'] for s in test_df['rouge_scores']]) / len(test_df),\n",
    "    'rouge-l': sum([s['rouge-l']['f'] for s in test_df['rouge_scores']]) / len(test_df),\n",
    "}\n",
    "\n",
    "print(f\"Average ROUGE-1 score: {avg_rouge_scores['rouge-1']:.2f}\")\n",
    "print(f\"Average ROUGE-2 score: {avg_rouge_scores['rouge-2']:.2f}\")\n",
    "print(f\"Average ROUGE-L score: {avg_rouge_scores['rouge-l']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline2: tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "\n",
    "# stop words \n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "train_matrix = vectorizer.fit_transform(train_df['body'])\n",
    "\n",
    "def extract_top_sentences(text, num_sentences=3):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "    sentence_scores = vectorizer.fit_transform(sentences).toarray().sum(axis=1)\n",
    "    \n",
    "    # top-ranked sentence\n",
    "    top_indices = sentence_scores.argsort()[-num_sentences:]\n",
    "    top_indices.sort()\n",
    "    \n",
    "    summary = [sentences[i] for i in top_indices]\n",
    "    return ' '.join(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['generated_summary_tfidf'] = test_df['body'].apply(lambda x: extract_top_sentences(x))\n",
    "\n",
    "# Apply extractive summarization using TF-IDF to the \"body\" column\n",
    "#df['body_extractive_summary_tfidf'] = df['body'].apply(lambda x: summarize_extractive_tfidf(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1 score: 0.22\n",
      "Average ROUGE-2 score: 0.04\n",
      "Average ROUGE-L score: 0.19\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from rouge import Rouge\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "rouge = Rouge()\n",
    "test_df['rouge_scores'] = test_df.apply(lambda row: rouge.get_scores(row['generated_summary_tfidf'], row['summary'])[0], axis=1)\n",
    "\n",
    "avg_rouge_scores = {\n",
    "    'rouge-1': sum([s['rouge-1']['f'] for s in test_df['rouge_scores']]) / len(test_df),\n",
    "    'rouge-2': sum([s['rouge-2']['f'] for s in test_df['rouge_scores']]) / len(test_df),\n",
    "    'rouge-l': sum([s['rouge-l']['f'] for s in test_df['rouge_scores']]) / len(test_df),\n",
    "}\n",
    "\n",
    "print(f\"Average ROUGE-1 score: {avg_rouge_scores['rouge-1']:.2f}\")\n",
    "print(f\"Average ROUGE-2 score: {avg_rouge_scores['rouge-2']:.2f}\")\n",
    "print(f\"Average ROUGE-L score: {avg_rouge_scores['rouge-l']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1 score: 0.22\n",
      "Average ROUGE-2 score: 0.04\n",
      "Average ROUGE-SU4 score: 0.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "avg_rouge_scores = {\n",
    "    'rouge-1': sum([s['rouge-1']['f'] for s in test_df['rouge_scores']]) / len(test_df),\n",
    "    'rouge-2': sum([s['rouge-2']['f'] for s in test_df['rouge_scores']]) / len(test_df),\n",
    "    'rouge-su4': sum([s['rouge-su4']['f'] for s in test_df['rouge_scores'] if 'rouge-su4' in s]) / len(test_df),\n",
    "}\n",
    "\n",
    "print(f\"Average ROUGE-1 score: {avg_rouge_scores['rouge-1']:.2f}\")\n",
    "print(f\"Average ROUGE-2 score: {avg_rouge_scores['rouge-2']:.2f}\")\n",
    "print(f\"Average ROUGE-SU4 score: {avg_rouge_scores['rouge-su4']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# centroid baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim.downloader as api\n",
    "\n",
    "model = api.load('word2vec-google-news-300')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# centroid of sentence vectors\n",
    "def compute_centroid(sentence_vectors):\n",
    "    return np.mean(sentence_vectors, axis=0)\n",
    "\n",
    "# cosine similarity between two vectors\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "def summarize_centroid(text, num_sentences=3):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    sentence_vectors = []\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence.lower())\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "        if not words:\n",
    "            continue\n",
    "        \n",
    "        sentence_vector = np.zeros((300,))\n",
    "        for word in words:\n",
    "            if word in model.key_to_index:\n",
    "                sentence_vector += model.get_vector(word)\n",
    "        sentence_vector /= len(words)\n",
    "        sentence_vectors.append(sentence_vector)\n",
    "    \n",
    "    centroid_vector = compute_centroid(sentence_vectors)\n",
    "    sentence_similarities = []\n",
    "    for sentence_vector in sentence_vectors:\n",
    "        similarity = cosine_similarity(sentence_vector, centroid_vector)\n",
    "        sentence_similarities.append(similarity)\n",
    "\n",
    "    top_indices = np.argsort(sentence_similarities)[-num_sentences:]\n",
    "    top_indices.sort()\n",
    "    \n",
    "    summary = [sentences[i] for i in top_indices]\n",
    "    return ' '.join(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['generated_summary_centroid'] = test_df['body'].apply(lambda x: summarize_centroid(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1 score: 0.17\n",
      "Average ROUGE-2 score: 0.03\n",
      "Average ROUGE-L score: 0.14\n"
     ]
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "test_df['rouge_scores'] = test_df.apply(lambda row: rouge.get_scores(row['generated_summary_centroid'], row['summary'])[0], axis=1)\n",
    "\n",
    "\n",
    "# Compute the average ROUGE scores across all papers in the test set\n",
    "avg_rouge_scores = {\n",
    "    'rouge-1': sum([s['rouge-1']['f'] for s in test_df['rouge_scores']]) / len(test_df),\n",
    "    'rouge-2': sum([s['rouge-2']['f'] for s in test_df['rouge_scores']]) / len(test_df),\n",
    "    'rouge-l': sum([s['rouge-l']['f'] for s in test_df['rouge_scores']]) / len(test_df),\n",
    "}\n",
    "\n",
    "# Print the average ROUGE scores\n",
    "print(f\"Average ROUGE-1 score: {avg_rouge_scores['rouge-1']:.2f}\")\n",
    "print(f\"Average ROUGE-2 score: {avg_rouge_scores['rouge-2']:.2f}\")\n",
    "print(f\"Average ROUGE-L score: {avg_rouge_scores['rouge-l']:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-trained T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('t5-small')\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-small')\n",
    "max_length = 50\n",
    "\n",
    "def summarize_abstractive(text):\n",
    "    segments = [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
    "    \n",
    "    # Generate summary\n",
    "    summaries = []\n",
    "    for segment in segments:\n",
    "        input_ids = tokenizer.encode(\"summarize: \" + segment, return_tensors='pt', truncation=True, max_length=max_length)\n",
    "        summary_ids = model.generate(input_ids, max_length=max_length, early_stopping=True)\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    summary = ' '.join(summaries)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['generated_summary_new'] = test_df['body'].apply(lambda x: summarize_abstractive(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from rouge import Rouge\n",
    "rouge = Rouge()\n",
    "test_df['rouge_scores'] = test_df.apply(lambda row: rouge.get_scores(row['generated_summary_new'], row['abstract'])[0], axis=1)\n",
    "\n",
    "# Compute the average ROUGE scores across all papers in the test set\n",
    "avg_rouge_scores = {\n",
    "    'rouge-1': sum([s['rouge-1']['f'] for s in test_df['rouge_scores']]) / len(test_df),\n",
    "    'rouge-2': sum([s['rouge-2']['f'] for s in test_df['rouge_scores']]) / len(test_df),\n",
    "    'rouge-l': sum([s['rouge-l']['f'] for s in test_df['rouge_scores']]) / len(test_df),\n",
    "}\n",
    "\n",
    "print(f\"Average ROUGE-1 score: {avg_rouge_scores['rouge-1']:.2f}\")\n",
    "print(f\"Average ROUGE-2 score: {avg_rouge_scores['rouge-2']:.2f}\")\n",
    "print(f\"Average ROUGE-L score: {avg_rouge_scores['rouge-l']:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
